hydra:
  output_subdir: null
  run:  
    dir: .

encoder:
  text_encoder: 'openai/clip-vit-large-patch14'
  image_encoder: 'openai/clip-vit-large-patch14'

opt:
  name: AdamW
  lr: 0.001
  weight_decay: 5e-4
sche:
  name: DummyLR

num_epoch: 1
batch_size: 128
seed: 2025
model: MLP
dataset: HateMM
type: default
patience: 5
device: cuda:0
lambda_cluster: 0.38
lambda_pre: 52
k: 63
soft_threshold_alpha: 35
cluster_percentile: 0.2
lambda_diversity_intra: 0.80
entropy_threshold: 0.45
adapt_steps: 6

pretrained_model: '/source_model/MHClipEN.pth'
